{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "In this notebook, we quickly demonstrate how to boost the performance of a deep-learning based QA system with our approach to adaptive information retrieval. We are using the publicly available [DrQA](https://github.com/facebookresearch/DrQA) implementation as a showcase. Nonetheless, our results are generalizable to any deep QA system. \n",
    "\n",
    "For the motivation and theoretical background, please refer to our paper. \n",
    "\n",
    "If you find this helpful, please consider citing us:\n",
    "\n",
    "```\n",
    "@inprocidings{kratzwald2018adaptive, \n",
    "  title={Adaptive Document Retrieval for Deep Question Answering},\n",
    "  author={Kratzwald, Bernhard and Feuerriegel, Stefan},\n",
    "  booktitle={Empirical Methods in Natural Language Processing (EMNLP)},\n",
    "  year={2018}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate training data\n",
    "\n",
    "Before training the logistic regression model, we need to collect training data. For every query in a given dataset (we used SQuAD-v1.1-train in our paper), we write the (possibly normalized) confidence scores for every of the top-n documents as well as the position of the first document that contains the ground-truth answer to a csv file as follows:\n",
    "`score-top-1, score-top-2, ... , score-top-n, pos` \n",
    "\n",
    "It is better to choose a large n here since the model will learn the cut-off between 0 and n. As a rule of thumb we choose n to be 25 for document-based information retrieval and 250 for paragraph based information retrieval. If no answer was found within the first n documents, we set pos to n later. \n",
    "\n",
    "To generate training data for the DrQA system follow these four steps:\n",
    "\n",
    "I) (Optional) normalized scores of the ir module.\n",
    "At the end of method `closest_docs(self, query, k=1)` in `drqa/retriever/tf-idf-ranker.py` add the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores = doc_scores/np.sum(doc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) Replace the method `get_score` the file `script/retriever/eval.py` by the method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(answer_doc, match):\n",
    "    \"\"\"Search through all the top docs to see if they have the answer.\"\"\"\n",
    "    answer, (doc_ids, doc_scores) = answer_doc\n",
    "    pos = 0\n",
    "    for doc_id in doc_ids:\n",
    "        pos += 1\n",
    "        if has_answer(answer, doc_id, match):\n",
    "            return (pos, doc_scores)\n",
    "    return (-1, doc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III) In the `__main__` of the same script replace the lines following `scores = processes.map(get_score_partial, answers_docs)` by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_file, \"w\") as f:\n",
    "    for pos, score in scores:\n",
    "        f.write('{},{}\\n'.format(pos, \n",
    "                                  np.array2string(score, separator=',', \n",
    "                                                  max_line_width=999999)[1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV) To generate training data for the squad dataset and the top 25 documents you simply have to call: \n",
    "\n",
    "`python script/retriever/eval.py path_to_squad_dataset --top-n 25` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model \n",
    "\n",
    "Before training the model, we recommend splitting it into a train/test fraction to get a feeling for the strength of your classifier.  \n",
    "\n",
    "You can train the model using pytorch or tensorflow. Alternatively, you can use the [mord](https://pythonhosted.org/mord/) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mord\n",
    "reg = mord.OrdinalRidge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto')\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our paper we didn't vary alpha from it's defaul 1. You will see that lower alphas lead to a more expanded distribution of the cutoff point while bigger alphas will narrow the distribtuion. To visualize this better you can predictic and count the cutoff points for your test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_dev)\n",
    "b = 1\n",
    "print(np.bincount(y_pred.astype(np.int32)+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the model to a file, so we can integrate it into the QA system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement the trained model\n",
    "\n",
    "To use the trained model in the DrQA pipeline you first have to load the model in the `__init__` method of the `tf-idf-ranker.py` and then alter the `closest_docs` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def closest_docs(self, query, k=1):\n",
    "        \"\"\"Closest docs by dot product between query and documents\n",
    "        in tfidf weighted word vector space.\n",
    "        \"\"\"\n",
    "        spvec = self.text2spvec(query)\n",
    "        res = spvec * self.doc_mat\n",
    "\n",
    "        if len(res.data) <= k:\n",
    "            o_sort = np.argsort(-res.data)\n",
    "        else:\n",
    "            o = np.argpartition(-res.data, k)[0:k]\n",
    "            o_sort = o[np.argsort(-res.data[o])]\n",
    "\n",
    "        doc_scores = res.data[o_sort]\n",
    "        doc_scores = doc_scores/np.sum(doc_scores) # THIS LINE IS ONLY NECCESSARY IF YOUR TRAINING DATA IS USING NORMALIZED SCORES\n",
    "\n",
    "        x = np.zeros([1,25])\n",
    "        x[0,0:len(doc_scores)]=doc_scores\n",
    "\n",
    "        b = 1\n",
    "        \n",
    "        y = self.model.predict(x)[0].astype(np.int32) + b\n",
    "\n",
    "        doc_ids = [self.get_doc_id(i) for i in res.indices[o_sort]]\n",
    "        return doc_ids[0:y], doc_scores[0:y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the system now with `--top-n 25` it will automatically predict the cutoffpoint between the first 25 documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
